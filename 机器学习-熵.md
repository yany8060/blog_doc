### 定义
从信息的角度来说，熵(Entropy)是一种从定量的角度来衡量信息(Information) 多少的指标。简单的说，就是信息所包含的不确定性的大小度量。一个信息所包含的事件的不确定性越大，它所含的信息就越多。我的理解是在这个信息还未获得时，这个信息所描述的事件的不确定性的大小。


### 信息量
当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。

假设X是一个离散型随机变量，其取值集合为χ,概率分布函数p(x)=Pr(X=x),x∈χ,则定义事件X=x0的信息量为：
$$ h(x_0) = -logP(x_0)) $$

### 熵
熵用来表示所有信息量的期望，即：$$ H(X)=-\sum_{i=1}^n p(x_i)log(p(x_i)) $$